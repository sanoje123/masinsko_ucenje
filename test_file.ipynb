{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization formula: z = (x - mean) / standard_deviation\n",
    "\n",
    "class Standardiser():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.size = X.shape[1]\n",
    "        self.mean = X.mean(axis=0)\n",
    "        self.std = X.std(axis=0)\n",
    "        return self     #returns self, which allows method chaining\n",
    "\n",
    "    def transform(self,X):\n",
    "        if X.shape[1] != self.size:\n",
    "            raise Exception(\"Wrong array dimensions!\")\n",
    "        return (X-self.mean)/self.std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, Y):\n",
    "    data_num = np.arange(X.shape[0])\n",
    "    np.random.shuffle(data_num)\n",
    "    return X[data_num], Y[data_num]\n",
    "\n",
    "def train_test_split(X, Y, test_size=0.5, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splits dataset into training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : array-like\n",
    "        Feature dataset.\n",
    "    y : array-like\n",
    "        Target values.\n",
    "    test_size : float or int, default=0.5\n",
    "        - If float (0 < test_size < 1), it represents the proportion of the dataset to include in the test split.\n",
    "        - If int (1 <= test_size < len(y)), it represents the absolute number of test samples.\n",
    "    shuffle : bool, default=True\n",
    "        If True, shuffles data before splitting.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    X_train, X_test, y_train, y_test : array-like\n",
    "        Split feature and target sets for training and testing.\"\"\"\n",
    "\n",
    "    if shuffle:\n",
    "        X, Y = shuffle_data(X, Y)\n",
    "    if test_size <1 :\n",
    "        train_ratio = len(Y) - int(len(Y) *test_size)\n",
    "        X_train, X_test = X[:train_ratio], X[train_ratio:]\n",
    "        Y_train, Y_test = Y[:train_ratio], Y[train_ratio:]\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    elif test_size in range(1,len(Y)):\n",
    "        X_train, X_test = X[test_size:], X[:test_size]\n",
    "        Y_train, Y_test = Y[test_size:], Y[:test_size]\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "\n",
    "def cross_val_split(X, Y, cv):\n",
    "    l = len(X)\n",
    "    size = int(l/cv)\n",
    "    X_splits, Y_splits = [], []\n",
    "\n",
    "    for i in range(cv):\n",
    "        X_temp = X[size * i : size * (i + 1)]\n",
    "        Y_temp = Y[size * i : size * (i + 1)]\n",
    "        X_splits.append(X_temp)\n",
    "        Y_splits.append(Y_temp)\n",
    "\n",
    "    # Stack all subsets along a new axis\n",
    "    X_split = np.stack(X_splits, axis=0)\n",
    "    Y_split = np.stack(Y_splits, axis=0)\n",
    "\n",
    "    return X_split, Y_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWLR():\n",
    "    def __init__(self,tau):\n",
    "        self.tau = tau\n",
    "\n",
    "    def fit(self,X, Y):\n",
    "        self.dim = X.ndim\n",
    "        self.X = self._add_ones(X)\n",
    "        self.X_T = self.X.T\n",
    "        self.Y = Y\n",
    "\n",
    "    def predict(self, P):\n",
    "        if P.ndim != self.dim:\n",
    "            P = [P]\n",
    "        P = self._add_ones(P)\n",
    "        out = []\n",
    "        for p in P:\n",
    "            # Vectorized calculation of weights\n",
    "            W = np.exp(-np.sum((self.X - p) ** 2, axis=1) / (2 * self.tau ** 2))    #Do the sume because we have many predictors\n",
    "            W_diag = np.diag(W)\n",
    "\n",
    "            # Calculate theta using vectorized operations\n",
    "            XT_W_X = self.X_T @ W_diag @ self.X\n",
    "            XT_W_Y = self.X_T @ W_diag @ self.Y\n",
    "            theta = np.linalg.solve(XT_W_X, XT_W_Y) #Solving XT_W_X @ theta = XT_W_Y is faster and more stable than calculating np.linalg.inv(XT_W_X) @ XT_W_Y\n",
    "\n",
    "            # Make prediction\n",
    "            out.append((theta @ p).squeeze())\n",
    "        return np.array(out)\n",
    "\n",
    "    def change_tau(self,tau):\n",
    "        self.tau = tau\n",
    "\n",
    "    def _add_ones(self, X):\n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.array(X)\n",
    "        return np.concatenate((np.ones((len(X),1)), X.reshape(len(X),-1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error on training data:\n",
      "5.27\n",
      "Mean square error on test data:\n",
      "5.87\n"
     ]
    }
   ],
   "source": [
    "#Read the data\n",
    "\n",
    "dile_name = 'data-reg.csv'\n",
    "# dile_name = 'C:/Users/USER/Desktop/Masinsko/Domaci/1/data.csv'\n",
    "raw_data = pd.read_csv(dile_name, header=None).to_numpy()\n",
    "\n",
    "X = raw_data[:,:5]\n",
    "Y = raw_data[:,-1]\n",
    "\n",
    "\n",
    "# print(f\"Mean of all predictors:\\n{X.mean(axis=0)}\")\n",
    "# print(f\"Mean of all predictors standardised:\\n{X_scaled.mean(axis=0)}\")\n",
    "\n",
    "# print(raw_data[1:5,:])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, shuffle=False)\n",
    "\n",
    "scaler = Standardiser().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# X_split, Y_split = cross_val_split(X_train, Y_train, 4)\n",
    "\n",
    "tau_opt = 1.7\n",
    "lwlr = LWLR(tau_opt)\n",
    "lwlr.fit(X_train_scaled,Y_train)\n",
    "\n",
    "# Generating prediction\n",
    "prediction_train = lwlr.predict(X_train_scaled)\n",
    "prediction_test = lwlr.predict(X_test_scaled)\n",
    "\n",
    "# Calculating mean square error\n",
    "mse_train = np.sqrt((np.square(prediction_train - Y_train)).mean())\n",
    "mse_test = np.sqrt((np.square(prediction_test - Y_test)).mean())\n",
    "\n",
    "print(\"Mean square error on training data:\")\n",
    "print(\"{:.2f}\".format(mse_train))\n",
    "print(\"Mean square error on test data:\")\n",
    "print(\"{:.2f}\".format(mse_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342,)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_scaled,axis=1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masinsko_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
