{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization formula: z = (x - mean) / standard_deviation\n",
    "\n",
    "class Standardiser():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.size = X.shape[1]\n",
    "        \n",
    "        self.mean = X.mean(axis=0)\n",
    "        self.std = X.std(axis=0)\n",
    "        return self     #returns self, which allows method chaining\n",
    "    \n",
    "    def transform(self,X):\n",
    "        if X.shape[1] != self.size:\n",
    "            raise Exception(\"Wrong array dimensions!\")\n",
    "        return (X-self.mean)/self.std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, Y):\n",
    "\n",
    "    data_num = np.arange(X.shape[0])\n",
    "    np.random.shuffle(data_num)\n",
    "    return X[data_num], Y[data_num]\n",
    "\n",
    "def train_test_split(X, Y, test_size=0.5, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splits dataset into training and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : array-like\n",
    "        Feature dataset.\n",
    "    y : array-like\n",
    "        Target values.\n",
    "    test_size : float or int, default=0.5\n",
    "        - If float (0 < test_size < 1), it represents the proportion of the dataset to include in the test split.\n",
    "        - If int (1 <= test_size < len(y)), it represents the absolute number of test samples.\n",
    "    shuffle : bool, default=True\n",
    "        If True, shuffles data before splitting.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    X_train, X_test, y_train, y_test : array-like\n",
    "        Split feature and target sets for training and testing.\"\"\"\n",
    "    \n",
    "    if shuffle:\n",
    "        X, Y = shuffle_data(X, Y)\n",
    "    if test_size <1 :\n",
    "        train_ratio = len(Y) - int(len(Y) *test_size)\n",
    "        X_train, X_test = X[:train_ratio], X[train_ratio:]\n",
    "        Y_train, Y_test = Y[:train_ratio], Y[train_ratio:]\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    elif test_size in range(1,len(Y)):\n",
    "        X_train, X_test = X[test_size:], X[:test_size]\n",
    "        Y_train, Y_test = Y[test_size:], Y[:test_size]\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "\n",
    "def cross_val_split(X, Y, cv):\n",
    "    l = len(X)\n",
    "    size = int(l/cv)\n",
    "    X_splits, Y_splits = [], []\n",
    "\n",
    "    for i in range(cv):\n",
    "        X_temp = X[size * i : size * (i + 1)]\n",
    "        Y_temp = Y[size * i : size * (i + 1)]\n",
    "        \n",
    "        X_splits.append(X_temp)\n",
    "        Y_splits.append(Y_temp)\n",
    "\n",
    "    # Stack all subsets along a new axis\n",
    "    X_split = np.stack(X_splits, axis=0)\n",
    "    Y_split = np.stack(Y_splits, axis=0)\n",
    "\n",
    "    return X_split, Y_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWLR():\n",
    "    def __init__(self,tau):\n",
    "        self.tau = tau\n",
    "  \n",
    "    def fit(self,X, Y):\n",
    "        self.dim = X.ndim\n",
    "        self.X = self._add_ones(X)\n",
    "        self.X_T = X.T\n",
    "        self.Y = Y\n",
    "\n",
    "    def predict(self, P):\n",
    "        P = self._add_ones(P)\n",
    "        out = []\n",
    "        for p in P:\n",
    "            # Vectorized calculation of weights\n",
    "            W = np.exp(-np.sum((self.X - p) ** 2, axis=1) / (2 * self.tau ** 2))    #Do the sume because we have many predictors\n",
    "            W_diag = np.diag(W)\n",
    "\n",
    "            # Calculate theta using vectorized operations\n",
    "            XT_W_X = self.X_T @ W_diag @ self.X\n",
    "            XT_W_Y = self.X_T @ W_diag @ self.Y\n",
    "            theta = np.linalg.solve(XT_W_X, XT_W_Y) #Solving XT_W_X @ theta = XT_W_Y is faster and more stable than calculating np.linalg.inv(XT_W_X) @ XT_W_Y\n",
    "\n",
    "            # Make prediction\n",
    "            out.append((theta @ p).squeeze())\n",
    "        return np.array(out)\n",
    "\n",
    "    def change_tau(self,tau):\n",
    "        self.tau = tau\n",
    "\n",
    "    def _add_ones(self, X):\n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.array(X)\n",
    "        return np.concatenate((np.ones((len(X),1)), X.reshape(len(X),-1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of all predictors[0.20131508 8.97294864 0.13104092 0.11286003 0.22287212]\n",
      "Mean of all predictors standardised[-2.53208760e-17  0.00000000e+00 -7.79103877e-18  4.28507132e-17\n",
      " -1.81790905e-17]\n",
      "[[ 1.42724753e+00  1.49424745e+02  3.43088589e+00  4.66607724e+00\n",
      "   9.00486546e+00  2.97000000e+01]\n",
      " [ 2.18171598e+00 -7.07277125e+01 -2.59226200e-01 -2.36445576e+00\n",
      "   2.37749440e+00  1.78000000e+01]\n",
      " [ 5.95405824e+00  6.17248717e+02  1.08111101e+01  6.89822116e+00\n",
      "   1.27327617e+01  2.68000000e+01]\n",
      " [ 4.22955892e+00  2.45741445e+02 -3.94933829e+00  5.22799998e+00\n",
      "   2.79170509e+00  1.66000000e+01]]\n"
     ]
    }
   ],
   "source": [
    "#Read the data\n",
    "\n",
    "dile_name = 'data-reg.csv'\n",
    "raw_data = pd.read_csv(dile_name, header=None).to_numpy()\n",
    "\n",
    "X = raw_data[:,:5]\n",
    "Y = raw_data[:,-1]\n",
    "\n",
    "print(f\"Mean of all predictors{X.mean(axis=0)}\")\n",
    "\n",
    "scaler = Standardiser().fit(X)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "print(f\"Mean of all predictors standardised{X_scaled.mean(axis=0)}\")\n",
    "\n",
    "print(raw_data[1:5,:])\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_split, Y_split = cross_val_split(X_train, Y_train, 4)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_scaled,axis=1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masinsko_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
